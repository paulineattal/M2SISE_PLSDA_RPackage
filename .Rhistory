Y.iter = Y.init #Y0
#on déroule l'algorithme NIPALS pour calculer les composantes de X et Y
for(k in 1:ncomp){
#u = premiere colonne de Yk-1
u <- as.matrix(Y.iter[,1])
u[is.na(u)] <- 0
#initialisations
w.old <- 1/rep(sqrt(ncol(X)), ncol(X))
w <- vector("numeric", length = ncol(X))
iter <- 1
diff <- 1
if (na.X)
{
X.aux <- X.iter
X.aux[is.na.X] <- 0
}
#on boucle jusqu'à ce que wk converge
while (diff > tol & iter <= max.iter){
if (na.X)
{
#calcul des poids w de X
w <- crossprod(X.aux, u)
Th <- drop(u) %o% nc.ones
Th[is.na.X] <- 0
u.cross <- crossprod(Th)
w <- w / diag(u.cross)
w <- w / drop(sqrt(crossprod(w)))
#calcul de la composante u de Xk-1
u <- X.aux %*% w
M <- drop(w) %o% nr.ones
M[t(is.na.X)] <- 0
ph.cross <- crossprod(M)
u <- u / diag(ph.cross)
} else {
#wk
w <- crossprod(X.iter, u) / drop(crossprod(u))
#normer a 1
w <- w / drop(sqrt(crossprod(w)))
#calcul de la composante u de Xk-1
#pk ca fait 1 le drop(crossprod(w)) ???
t <- X.iter %*% w / drop(crossprod(w)) #tk
}
#calcul des poids de Yk-1
q <- crossprod(Y.iter,t)/drop(crossprod(t)) #qk
#prochiane colonne de Yk-1
u<-Y.iter  %*% q / drop(crossprod(q)) #x valeurs pour les x modalités
diff <- drop(sum((w - w.old)^2, na.rm = TRUE))
w.old <- w
iter = iter + 1
}
if (iter > max.iter){
message(paste("Maximum number of iterations reached for comp: ", k))
}
#SVD de X
#eigTx[k] <- sum(t * t, na.rm = TRUE)
#P
P=crossprod(X.iter,t)/drop(crossprod(t))
#mise à jour des X
X.iter <- X.iter - t %*% t(P)
#mise à jour des Y
Y.iter <- Y.iter - t%*%t(q)
#stocker les resultats pour la sortie
#matrice des composantes de X
Tx[,k] <- t
#matrice des composantes de Y
U[,k] <- u
#matrice des poids des composante X
W[,k] <- w
#matrice des poids des composante Y
Q[,k] <- q
}
#eigTx <- sqrt(eigTx)
Rx <- cor(X.init,Tx)^2
colnames(Rx) <- paste(rep("Comp",ncomp), 1:ncomp, sep=" ")
if (ncomp == 1) {
Var.Explained.X <- rbind(Rx,Redundancy=mean(Rx))
Rx.cum <- as.matrix(apply(Rx,1,cumsum))
Var.Explained.X.Cum <- rbind(Rx.cum,Redundancy=mean(Rx.cum))
} else {
Var.Explained.X <- rbind(Rx,Redundancy=colMeans(Rx))
Rx.cum <- t(apply(Rx,1,cumsum))
Var.Explained.X.Cum <- rbind(Rx.cum,Redundancy=colMeans(Rx.cum))
}
# For Y (not cumulative and cumulated)
Ry <- cor(Y.init,Tx)^2
colnames(Ry) <- paste(rep("Comp",ncomp), 1:ncomp, sep=" ")
if (ncomp == 1) {
Var.Explained.Y <- rbind(Ry, Redundancy=mean(Ry))
Ry.cum <- as.matrix(apply(Ry, 1, cumsum))
Var.Explained.Y.Cum <- rbind(Ry.cum, Redundancy=mean(Ry.cum))
} else {
Var.Explained.Y <- rbind(Ry, Redundancy=colMeans(Ry))
Ry.cum <- t(apply(Ry, 1, cumsum))
Var.Explained.Y.Cum <- rbind(Ry.cum, Redundancy=colMeans(Ry.cum))
}
Var.Explained.Y
Ry.cum
Var.Explained.Y.Cum
Var.Explained.X
Rx.cum
Var.Explained.X.Cum
###########################################
#critere a maximiser
#=somme des carrés des covariances entre composante et chacune des variables réponses
#R2 <- cor(y, Tx)^2 #
res <- list("comp_X"= Tx,
"poid_X" = W,
"comp_Y" = U,
"poid_Y" = Q,
"Y.iter" = Y.iter
#"Xscores" = eigTx,
#"R2" = R2,
#"Coeffs"=coeffs
)
class(res)<-"PLSDA"
return(res)
}
#on exécute le modèle sur les données d'appprentissage
fit<-fit(Y~., train, ncomp = j)
#on fait la prédiction sur X.test
pred <- plsda.predict(fit, X.test)
pred
y2=unique(y2)
y2=unique(pred)
y
y2
#formatage de y
y <- as.factor(as.vector(y))
#formatage de y
y2 <- as.factor(as.vector(y2))
y
y2
#nombre de modalités
n.mod <- levels(y)
mod <- as.factor(as.vector(mod))
mod <- as.factor(as.vector(y2))
mod
y2
y
setequal(y,y2)
y
mod
setequal(mod, y)
setdiff(y, mod)
# Matrice d'indicatrice 0/1
dum<-sapply(n.mod,function(x){ifelse(y==x,1,0)})
dum
y
tem=y
mod=y
y=temp
y=tem
mod
y
tem
pred
y=unique(pred)
y
#formatage de y
y <- as.factor(as.vector(y))
mod <- as.factor(as.vector(mod))
mod
y
setequal(mod, y)
setdiff(y, mod)
setdiff(mod, y)
sapply(n.mod,function(x){ifelse(y==x,1,0)})
y
n.mod
#fonction pour creer des dummies
#a partir d'une variable de type factor
#codage disjonctif complet
plsda.dummies<-function(y, mod=TRUE){
#formatage de y
y <- as.factor(as.vector(y))
#nombre de modalités
if (!mod){
mod <- as.factor(as.vector(mod))
n.mod <- levels(mod)
}else{
n.mod <- levels(y)
}
# Matrice d'indicatrice 0/1
dum<-sapply(n.mod,function(x){ifelse(y==x,1,0)})
#renvoyer sous la forme de data frame
return(as.data.frame(dum))
}
pred
Y.test
test_dum=plsda.dummies(pred,Y.test)
#fonction pour creer des dummies
#a partir d'une variable de type factor
#codage disjonctif complet
plsda.dummies<-function(y, mod=TRUE){
#formatage de y
y <- as.factor(as.vector(y))
#nombre de modalités
if (mod!=TRUE){
mod <- as.factor(as.vector(mod))
n.mod <- levels(mod)
}else{
n.mod <- levels(y)
}
# Matrice d'indicatrice 0/1
dum<-sapply(n.mod,function(x){ifelse(y==x,1,0)})
#renvoyer sous la forme de data frame
return(as.data.frame(dum))
}
test_dum=plsda.dummies(pred,Y.test)
test=TRUE
if(test==TRUE){print("a")}
test="p"
if(test!=TRUE){print("a")}
#fonction pour creer des dummies
#a partir d'une variable de type factor
#codage disjonctif complet
plsda.dummies<-function(y, mod=TRUE){
print(mod)
#formatage de y
y <- as.factor(as.vector(y))
#nombre de modalités
if (mod!=TRUE){
mod <- as.factor(as.vector(mod))
n.mod <- levels(mod)
}else{
n.mod <- levels(y)
}
# Matrice d'indicatrice 0/1
dum<-sapply(n.mod,function(x){ifelse(y==x,1,0)})
#renvoyer sous la forme de data frame
return(as.data.frame(dum))
}
test_dum=plsda.dummies(pred,Y.test)
test = mod
if(test!=TRUE){print("a")}
#fonction pour creer des dummies
#a partir d'une variable de type factor
#codage disjonctif complet
plsda.dummies<-function(y, mod="TRUE"){
print(mod)
#formatage de y
y <- as.factor(as.vector(y))
#nombre de modalités
if (mod!="TRUE"){
mod <- as.factor(as.vector(mod))
n.mod <- levels(mod)
}else{
n.mod <- levels(y)
}
# Matrice d'indicatrice 0/1
dum<-sapply(n.mod,function(x){ifelse(y==x,1,0)})
#renvoyer sous la forme de data frame
return(as.data.frame(dum))
}
test_dum=plsda.dummies(pred,Y.test)
pred
pred
length(pred)
length(Y.test)
#fonction pour creer des dummies
#a partir d'une variable de type factor
#codage disjonctif complet
plsda.dummies<-function(y, mod=TRUE{
#fonction pour creer des dummies
#a partir d'une variable de type factor
#codage disjonctif complet
plsda.dummies<-function(y, mod=TRUE){
print(mod)
#formatage de y
y <- as.factor(as.vector(y))
#nombre de modalités
if (length(mod)!=0){
mod <- as.factor(as.vector(mod))
n.mod <- levels(mod)
}else{
n.mod <- levels(y)
}
# Matrice d'indicatrice 0/1
dum<-sapply(n.mod,function(x){ifelse(y==x,1,0)})
#renvoyer sous la forme de data frame
return(as.data.frame(dum))
}
test_dum=plsda.dummies(pred,Y.test)
test_dum
X
data
sel_var = FALSE
if (length(sel_var)!=0){
print("oui")
}
sel_var = TRUE
if (length(sel_var)!=0){
print("oui")
}
sel_var = ""
if (length(sel_var)!=0){
print("oui")
}
sel_var = NA
if (length(sel_var)!=0){
print("oui")
}
length(sel_var)
mod=NA
!is.NA(mod)
is.na(mod)
!is.na(mod)
data
#fonction pour le test du retrait d'une variable numéro j
#SW : matrice de covariance intra
#ST : matrice de covariance totale
#N : nombre d'observations
#K : nombre de classes
#precLW : valeur du lambda avant le retrait
test.retrait <- function(j,SW,ST,N,K,precLW){
#nombre de variables à cette étape
nbVar <- ncol(SW)
#lamdba
lambada <- det(SW[-j,-j])/det(ST[-j,-j])
#ddl
ddlNum <- K - 1
ddlDenom <- N - K - nbVar + 1
#tatistique de test
f <- ddlDenom / ddlNum * (lambada / precLW - 1)
#p-value
pvalue <- pf(f,ddlNum,ddlDenom,lower.tail=FALSE)
#renvoyer sous forme de vecteur
return(c(lambada,f,pvalue))
}
#fonction de selection backward
#alpha : risque pour piloter la sélection
sel.backward <- function(data,alpha=0.05){
XTrain <- as.matrix(model.matrix(formula, data = data)[,-1])
yTrain <- as.factor(model.response(model.frame(formula, data = data)))
n <- nrow(data) # nombre d'observations
K <- nlevels(yTrain) # nombre de classes
p <- ncol(XTrain) # nombre de variables
n_k <- table(yTrain) #effectifs des classes
lstInit <- colnames(XTrain)
#calculs des matrices de var.covariance pour wilks
#matrice initiale de covariance totale
Vb <- (n-1)/n * cov(XTrain)
#matrice initiale de covariance intra
Wb <- (1.0/n)*Reduce("+",lapply(levels(yTrain),function(niveau){(n_k[niveau]-1)*cov(XTrain[yTrain==niveau,])}))
#lambda de Wilks initial (incluant les p variables)
lambda_depart <- det(Wb)/det(Vb)
#matrice pour récupérer les résultats à chaque étape
mResult <- matrix(0,nrow=0,ncol=3)
colnames(mResult) <- c('Lambda','F-Test','p-value')
#liste des variables retirées
lstvarDel <- c()
#boucle de recherche
repeat{
#resultats d'une étape
res <- t(sapply(1:ncol(Wb),test.retrait,SW=Wb,ST=Vb,N=n,K=K,precLW=lambda_depart))
#affichage pour un mode "verbose"
aff <- res
colnames(aff) <- c("Lambda","F","p-value")
rownames(aff) <- lstInit
print(aff)
#trouver la ligne qui maximise la p-value
id <- which.max(res[,3])
#vérifier si on doit faire un retrait (comparer p-value à alpha), sinon on arrete
if (res[id,3] > alpha){
#nom de la variable à retirer
lstvarDel <- c(lstvarDel,lstInit[id])
#rajouter la ligne de résultats (lambda,F-Test,p-value)
mResult <- rbind(mResult,res[id,])
#retirer
lstInit <- lstInit[-id]
#si on n'a plus de variables a retirer, on arrete
if (length(lstInit) == 0){
break
} else {
#retirer les colonnes des matrices
Wb <- Wb[-id,-id]
Vb <- Vb[-id,-id]
#mise a jour du lambda
lambda_depart <- res[id,1]
}
} else {
break
}
}
#renvoi les resultats sous la forme d'un data frame
resultats <- data.frame(var=lstvarDel,mResult)
return(resultats)
}
#fonction de selection backward
#alpha : risque pour piloter la sélection
backward <- function(data,alpha=0.05){
XTrain <- as.matrix(model.matrix(formula, data = data)[,-1])
yTrain <- as.factor(model.response(model.frame(formula, data = data)))
n <- nrow(data) # nombre d'observations
K <- nlevels(yTrain) # nombre de classes
p <- ncol(XTrain) # nombre de variables
n_k <- table(yTrain) #effectifs des classes
lstInit <- colnames(XTrain)
#calculs des matrices de var.covariance pour wilks
#matrice initiale de covariance totale
Vb <- (n-1)/n * cov(XTrain)
#matrice initiale de covariance intra
Wb <- (1.0/n)*Reduce("+",lapply(levels(yTrain),function(niveau){(n_k[niveau]-1)*cov(XTrain[yTrain==niveau,])}))
#lambda de Wilks initial (incluant les p variables)
lambda_depart <- det(Wb)/det(Vb)
#matrice pour récupérer les résultats à chaque étape
mResult <- matrix(0,nrow=0,ncol=3)
colnames(mResult) <- c('Lambda','F-Test','p-value')
#liste des variables retirées
lstvarDel <- c()
#boucle de recherche
repeat{
#resultats d'une étape
res <- t(sapply(1:ncol(Wb),test.retrait,SW=Wb,ST=Vb,N=n,K=K,precLW=lambda_depart))
#affichage pour un mode "verbose"
aff <- res
colnames(aff) <- c("Lambda","F","p-value")
rownames(aff) <- lstInit
print(aff)
#trouver la ligne qui maximise la p-value
id <- which.max(res[,3])
#vérifier si on doit faire un retrait (comparer p-value à alpha), sinon on arrete
if (res[id,3] > alpha){
#nom de la variable à retirer
lstvarDel <- c(lstvarDel,lstInit[id])
#rajouter la ligne de résultats (lambda,F-Test,p-value)
mResult <- rbind(mResult,res[id,])
#retirer
lstInit <- lstInit[-id]
#si on n'a plus de variables a retirer, on arrete
if (length(lstInit) == 0){
break
} else {
#retirer les colonnes des matrices
Wb <- Wb[-id,-id]
Vb <- Vb[-id,-id]
#mise a jour du lambda
lambda_depart <- res[id,1]
}
} else {
break
}
}
#renvoi les resultats sous la forme d'un data frame
resultats <- data.frame(var=lstvarDel,mResult)
return(resultats)
}
backward(data)
sel = backward(data = data)
sel
rbind(c("ISOP"), c("ACET"), c("ACAL"))
data
model.frame(formula, data = data)
model.response(model.frame(formula, data = data))
as.factor(model.response(model.frame(formula, data = data)))
data.frame(Y=model.response(model.frame(formula, data = data), data[, var_rm])
)
data.frame(Y=model.response(model.frame(formula, data = data), data[, var_rm])
)
data.frame(Y=model.response(model.frame(formula, data = data), data[, var_sel]))
var_sel
data.frame(Y=model.response(model.frame(formula, data = data), data[, sel]))
data.frame(Y=model.response(model.frame(formula, data = data), data[[], sel]]))
data.frame(Y=model.response(model.frame(formula, data = data), data[[, sel]]))
data.frame(Y=model.response(model.frame(formula, data = data), data[, c(sel)]))
srel
sel
rm = rbind(c("ISOP"), c("ACET"), c("ACAL"))
data.frame(Y=model.response(model.frame(formula, data = data), data[, rm]))
data[rm]
rm
class(rm)
as.vector(rm)
data.frame(Y=model.response(model.frame(formula, data = data), data[, as.vector(rm)]))
data.frame(Y=model.response(model.frame(formula, data = data), data[[, as.vector(rm)]]))
data.frame(Y=model.response(model.frame(formula, data = data), data[, as.vector(rm)]))
data[as.vector(rm)]
data[-as.vector(rm)]
data[-as.vector(rm)]
select(data,-as.vector(rm))
subset(data, select=-as.vector(rm))
rm=as.vector(rm)
rm
subset(data, select=rm)
subset(data, select=-rm)
subset(data, select=-c(rm)
)
data[,-rm]
data[,- rm]
rm
c(rm)
rm[1]
colnames(X)
colnames(data)
intersect(data, rm)
intersect(rm,data)
uniont(data, rm)
union(data, rm)
intersect(rm,colnames(data))
intersect(colnames(data),rm)
rm
colnames(data)
setdiff(colnames(data), rm)
data[setdiff(colnames(data), rm)]
y <- as.factor(model.response(model.frame(formula, data = data)))
y
K <- nlevels(y) # nombre de classes
levely <- levels(y)
K
levely
n_k <- table(y) #effectifs des classes
n_k

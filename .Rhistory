U <- data.frame(matrix(rep(0), nrow = nrow(Y.init), ncol=ncomp))
names(U) <-comp_names
#matrice des coefficients des composantes de Y
#/ coefficients de projection de des composantes Y
#/ poids de Y
Q <- data.frame(matrix(rep(NA), nrow = ncol(Y.init), ncol = ncomp))
rownames(Q) <- colnames(Y.init)
nc.ones <- rep(1, ncol(X))
nr.ones <- rep(1, nrow(X))
is.na.X <- is.na(X)
na.X <- any(is.na.X)
X.iter = X.init #X0
Y.iter = Y.init #Y0
#on déroule l'algorithme NIPALS pour calculer les composantes de X et Y
for(k in 1:ncomp){
#u = premiere colonne de Yk-1
u <- as.matrix(Y.iter[,1])
u[is.na(u)] <- 0
#initialisations
w.old <- 1/rep(sqrt(ncol(X)), ncol(X))
w <- vector("numeric", length = ncol(X))
iter <- 1
diff <- 1
if (na.X)
{
X.aux <- X.iter
X.aux[is.na.X] <- 0
}
#on boucle jusqu'à ce que wk converge
while (diff > tol & iter <= max.iter){
if (na.X)
{
#calcul des poids w de X
w <- crossprod(X.aux, u)
Th <- drop(u) %o% nc.ones
Th[is.na.X] <- 0
u.cross <- crossprod(Th)
w <- w / diag(u.cross)
w <- w / drop(sqrt(crossprod(w)))
#calcul de la composante u de Xk-1
u <- X.aux %*% w
M <- drop(w) %o% nr.ones
M[t(is.na.X)] <- 0
ph.cross <- crossprod(M)
u <- u / diag(ph.cross)
} else {
#wk
w <- crossprod(X.iter, u) / drop(crossprod(u))
#normer a 1
w <- w / drop(sqrt(crossprod(w)))
#calcul de la composante u de Xk-1
#pk ca fait 1 le drop(crossprod(w)) ???
t <- X.iter %*% w / drop(crossprod(w)) #tk
}
#calcul des poids de Yk-1
q <- crossprod(Y.iter,t)/drop(crossprod(t)) #qk
#prochiane colonne de Yk-1
u<-Y.iter  %*% q / drop(crossprod(q)) #x valeurs pour les x modalités
diff <- drop(sum((w - w.old)^2, na.rm = TRUE))
w.old <- w
iter = iter + 1
}
if (iter > max.iter){
message(paste("Maximum number of iterations reached for comp: ", k))
}
#SVD de X
#eigTx[k] <- sum(t * t, na.rm = TRUE)
#P
P=crossprod(X.iter,t)/drop(crossprod(t))
#mise à jour des X
X.iter <- X.iter - t %*% t(P)
#mise à jour des Y
Y.iter <- Y.iter - t%*%t(q)
#stocker les resultats pour la sortie
#matrice des composantes de X
Tx[,k] <- t
#matrice des composantes de Y
U[,k] <- u
#matrice des poids des composante X
W[,k] <- w
#matrice des poids des composante Y
Q[,k] <- q
}
#eigTx <- sqrt(eigTx)
Rx <- cor(X.init,Tx)^2
colnames(Rx) <- paste(rep("Comp",ncomp), 1:ncomp, sep=" ")
if (ncomp == 1) {
Var.Explained.X <- rbind(Rx,Redundancy=mean(Rx))
Rx.cum <- as.matrix(apply(Rx,1,cumsum))
Var.Explained.X.Cum <- rbind(Rx.cum,Redundancy=mean(Rx.cum))
} else {
Var.Explained.X <- rbind(Rx,Redundancy=colMeans(Rx))
Rx.cum <- t(apply(Rx,1,cumsum))
Var.Explained.X.Cum <- rbind(Rx.cum,Redundancy=colMeans(Rx.cum))
}
# For Y (not cumulative and cumulated)
Ry <- cor(Y.init,Tx)^2
colnames(Ry) <- paste(rep("Comp",ncomp), 1:ncomp, sep=" ")
if (ncomp == 1) {
Var.Explained.Y <- rbind(Ry, Redundancy=mean(Ry))
Ry.cum <- as.matrix(apply(Ry, 1, cumsum))
Var.Explained.Y.Cum <- rbind(Ry.cum, Redundancy=mean(Ry.cum))
} else {
Var.Explained.Y <- rbind(Ry, Redundancy=colMeans(Ry))
Ry.cum <- t(apply(Ry, 1, cumsum))
Var.Explained.Y.Cum <- rbind(Ry.cum, Redundancy=colMeans(Ry.cum))
}
Var.Explained.Y
Ry.cum
Var.Explained.Y.Cum
Var.Explained.X
Rx.cum
Var.Explained.X.Cum
###########################################
#critere a maximiser
#=somme des carrés des covariances entre composante et chacune des variables réponses
#R2 <- cor(y, Tx)^2 #
res <- list("comp_X"= Tx,
"poid_X" = W,
"comp_Y" = U,
"poid_Y" = Q,
"Y.iter" = Y.iter
#"Xscores" = eigTx,
#"R2" = R2,
#"Coeffs"=coeffs
)
class(res)<-"PLSDA"
return(res)
}
nipals.res <- plsda.nipals(X=X, y=ydum, ncomp=ncomp , max.iter=max.iter, tol=tol)
#ici on effectue la LDA pour la classification
#on l'a fait sur nos compossntes principales Th, obtenues en sorties de la PLS
Th <- nipals.res$comp_X
#effectif par classe
n_k <- table(y) #train
#nombre d'individus
n <- nrow(Th)
#nombre de modalite
K <- nlevels(y)
#nombre de variables desc
p <- ncol(Th)
#proportion par classe
pi_k <- n_k / n
#calcul des moyennes conditionelles - lignes = classes
mb_k <- as.matrix(aggregate(Th,list(y),mean)[,2:(p+1)])
#calcul des matrices de covariances conditionnelles
V_k <- by(as.matrix(Th),list(y),cov)
#matrice de covariance intra-classe W
#calculée à partir des matrices conditionnelles V_k
W <- 1/(n-K) * Reduce("+",lapply(levels(y),function(k){(n_k[k]-1)*V_k[[k]]}))
#inverse de la matrice W
invW <- solve(W)
#calcul des coefficients des variables akj
#pour la fonction de classement
coef_ <- t(mb_k %*% invW)
coef=coef_
coef
colnames(coef_) <- levels(y)
intercept_ <- log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
#revenir a toutes les var originelles
coef_ <- as.matrix(nipals.res$poid_X)%*%coef_
coef_ <- diag(1/apply(X.init, 2, sd)) %*% coef_
coef_
X
nipals.res <- plsda.nipals(X=X, y=ydum, ncomp=ncomp , max.iter=max.iter, tol=tol)
nipals.res
#ici on effectue la LDA pour la classification
#on l'a fait sur nos compossntes principales Th, obtenues en sorties de la PLS
Th <- nipals.res$comp_X
#effectif par classe
n_k <- table(y) #train
#nombre d'individus
n <- nrow(Th)
#nombre de modalite
K <- nlevels(y)
#nombre de variables desc
p <- ncol(Th)
#proportion par classe
pi_k <- n_k / n
#calcul des moyennes conditionelles - lignes = classes
mb_k <- as.matrix(aggregate(Th,list(y),mean)[,2:(p+1)])
#calcul des matrices de covariances conditionnelles
V_k <- by(as.matrix(Th),list(y),cov)
#matrice de covariance intra-classe W
#calculée à partir des matrices conditionnelles V_k
W <- 1/(n-K) * Reduce("+",lapply(levels(y),function(k){(n_k[k]-1)*V_k[[k]]}))
#inverse de la matrice W
invW <- solve(W)
#calcul des coefficients des variables akj
#pour la fonction de classement
coef_ <- t(mb_k %*% invW)
colnames(coef_) <- levels(y)
intercept_ <- log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
coef_
#revenir a toutes les var originelles
coef_ <- as.matrix(nipals.res$poid_X)%*%coef_
coef_ <- diag(1/apply(X.init, 2, sd)) %*% coef_
coef_
data
X
ncomp = 4
nipals.res <- plsda.nipals(X=X, y=ydum, ncomp=ncomp , max.iter=max.iter, tol=tol)
#ici on effectue la LDA pour la classification
#on l'a fait sur nos compossntes principales Th, obtenues en sorties de la PLS
Th <- nipals.res$comp_X
#effectif par classe
n_k <- table(y) #train
#nombre d'individus
n <- nrow(Th)
#nombre de modalite
K <- nlevels(y)
#nombre de variables desc
p <- ncol(Th)
#proportion par classe
pi_k <- n_k / n
#calcul des moyennes conditionelles - lignes = classes
mb_k <- as.matrix(aggregate(Th,list(y),mean)[,2:(p+1)])
#calcul des matrices de covariances conditionnelles
V_k <- by(as.matrix(Th),list(y),cov)
#matrice de covariance intra-classe W
#calculée à partir des matrices conditionnelles V_k
W <- 1/(n-K) * Reduce("+",lapply(levels(y),function(k){(n_k[k]-1)*V_k[[k]]}))
#inverse de la matrice W
invW <- solve(W)
#calcul des coefficients des variables akj
#pour la fonction de classement
coef_ <- t(mb_k %*% invW)
colnames(coef_) <- levels(y)
intercept_ <- log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
#revenir a toutes les var originelles
coef_ <- as.matrix(nipals.res$poid_X)%*%coef_
coef_ <- diag(1/apply(X.init, 2, sd)) %*% coef_
coef_
#calcul des coefficients des variables akj
#pour la fonction de classement
coef_ <- t(mb_k %*% invW)
coef_
y
ydum
getwd()
data = read_csv("/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data = read_excel("/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
library(readxl)
data = read_excel("/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data = read_excel("C:/Users/pauli/Documents/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data
formula = TYPE~.
#Récupération des X et Y
X <- as.matrix(model.matrix(formula, data = data)[,-1])
X.init <- X
y <- as.factor(model.response(model.frame(formula, data = data)))
#si data est a standardiser
if ((mean(apply(X,2,mean))>abs(1)) || (sum(sqrt(apply(X,2,var))) != ncol(X))){
X <- plsda.scale(X)
}
#codage disjonctif de la variable cible
ydum <- plsda.dummies(y)
nipals.res <- plsda.nipals(X=X, y=ydum, ncomp=ncomp , max.iter=max.iter, tol=tol)
#ici on effectue la LDA pour la classification
#on l'a fait sur nos compossntes principales Th, obtenues en sorties de la PLS
Th <- nipals.res$comp_X
#effectif par classe
n_k <- table(y) #train
#nombre d'individus
n <- nrow(Th)
#nombre de modalite
K <- nlevels(y)
#nombre de variables desc
p <- ncol(Th)
#proportion par classe
pi_k <- n_k / n
#calcul des moyennes conditionelles - lignes = classes
mb_k <- as.matrix(aggregate(Th,list(y),mean)[,2:(p+1)])
#calcul des matrices de covariances conditionnelles
V_k <- by(as.matrix(Th),list(y),cov)
#matrice de covariance intra-classe W
#calculée à partir des matrices conditionnelles V_k
W <- 1/(n-K) * Reduce("+",lapply(levels(y),function(k){(n_k[k]-1)*V_k[[k]]}))
#inverse de la matrice W
invW <- solve(W)
#calcul des coefficients des variables akj
#pour la fonction de classement
coef_ <- t(mb_k %*% invW)
coef_
colnames(coef_) <- levels(y)
intercept_ <- log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
#revenir a toutes les var originelles
coef_ <- as.matrix(nipals.res$poid_X)%*%coef_
coef_ <- diag(1/apply(X.init, 2, sd)) %*% coef_
coef_
-apply(X.init, 2, mean)
coef_
-apply(X.init, 2, mean) %*% coef_
rbind(c(-0.012, 0.004, 0.007,), c(0.001, 0.007, -0.006,),
c(-0.427, 0.228, 0.192),
c(-0.033, 0.000, 0.027),
c(0.018, -0.002, -0.013),
c(0.012, -0.071, 0.043),
c(0.003, -0.001, -0.002),
c(0.062, -0.079, 0.007))
rbind(c(-0.012, 0.004, 0.007,), c(0.001, 0.007, -0.006),
c(-0.427, 0.228, 0.192),
c(-0.033, 0.000, 0.027),
c(0.018, -0.002, -0.013),
c(0.012, -0.071, 0.043),
c(0.003, -0.001, -0.002),
c(0.062, -0.079, 0.007))
rbind(c(-0.012, 0.004, 0.007), c(0.001, 0.007, -0.006),
c(-0.427, 0.228, 0.192),
c(-0.033, 0.000, 0.027),
c(0.018, -0.002, -0.013),
c(0.012, -0.071, 0.043),
c(0.003, -0.001, -0.002),
c(0.062, -0.079, 0.007))
coef_
intercept_
intercept_ <- log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
intercept_
intercept %*% coef_
intercept_ %*% coef_
class(coef_)
class(intercept_)
as.matrix(intercept_)
t(as.matrix(intercept_))
coef_
coef_test = rbind(c(-0.012, 0.004, 0.007), c(0.001, 0.007, -0.006),
+ c(-0.427, 0.228, 0.192),
+ c(-0.033, 0.000, 0.027),
+ c(0.018, -0.002, -0.013),
+ c(0.012, -0.071, 0.043),
+ c(0.003, -0.001, -0.002),
+ c(0.062, -0.079, 0.007))
coef_test
as.vector(-apply(X.init, 2, mean) %*% coef_test)
as.vector(-apply(X.init, 2, mean) %*% coef_)
library(ggplot2)
nipals.res
pouette = as.data.frame(nipals.res$poid_X)
ggplot() +
geom_text(data=pouette, aes(x = X1, y = X2, label = rownames(pouette)), col = 'red') +
geom_segment(data=pouette, aes(x = 0, y = 0, xend = X1, yend = X2), arrow=arrow(length=unit(0.2,"cm")),alpha = 0.75, color = 'darkred')
pouettey = as.data.frame(nipals.res$poid_Y)
pouettey
pouettey = as.data.frame(nipals.res$comp_Y)
pouettey
y
ggplot(pouettey, aes(x=X1, y=X2, col = y, fill = y)) +
stat_ellipse(geom = "polygon", col= "black", alpha =0.5)+
geom_point(shape=21, col="black")
pouettey = as.data.frame(nipals.res$comp_Y)
pouettey
pouettey = as.data.frame(nipals.res$comp_X)
pouettey
pouettey = as.data.frame(nipals.res$comp_X, y)
nipals.res$comp_X,
y
as.data.frame(nipals.res$comp_X, as.factor(y))
nipals.res$comp_X
class(nipals.res$comp_X)
pouettey = as.data.frame(as.matrix(nipals.res$comp_X), as.matrix(y))
pouettey
pouettey = as.data.frame(as.matrix(nipals.res$comp_X), y=as.matrix(y))
pouettey
pouettey = cbind(nipals.res$comp_X, as.matrix(y))
pouettey
pouettey = cbind(nipals.res$comp_X, y=as.matrix(y))
pouettey
ggplot(pouettey, aes(x=X1, y=X2, col = y, fill = y)) +
stat_ellipse(geom = "polygon", col= "black", alpha =0.5)+
geom_point(shape=21, col="black")
ggplot(pouettey, aes(x=PC1, y=PC2, col = y, fill = y)) +
stat_ellipse(geom = "polygon", col= "black", alpha =0.5)+
geom_point(shape=21, col="black")
as.vector(-apply(X.init, 2, mean) %*% coef_) + log(pi_k)
as.vector(-apply(X.init, 2, mean) %*% coef_) + log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
X.init[,:4]
X.init[:4]
X.init[4]
X.init[,4]
X.init[4,]
X.init[,4]
X.init
X.init[[,4]]
fit <- function(formula, data,
ncomp = 4, #ici on peut mettre "CV"
sel_var = NA, #ici on peut mettre que backward
max.iter = 100,
tol = 1e-06)
{
#formula au bon type
if(plyr::is.formula(formula)==F){
stop("formula must be R formula !")
}
if (any(colSums(!is.na(data)) == 0) | any(rowSums(!is.na(data)) == 0 )){
stop("some rows or columns are entirely missing. ",
"Remove those before running pca.", call. = FALSE)
}
#if (sel_var){
#var_rm = backward(data)
#data = data[setdiff(colnames(data), as.vector(rm))]
#}
#Récupération des X et Y
X <- as.matrix(model.matrix(formula, data = data)[,-1])
X.init <- X
y <- as.factor(model.response(model.frame(formula, data = data)))
#si data est a standardiser
if ((mean(apply(X,2,mean))>abs(1)) || (sum(sqrt(apply(X,2,var))) != ncol(X))){
X <- plsda.scale(X)
}
#codage disjonctif de la variable cible
ydum <- plsda.dummies(y)
#if ncomp == "CV" {
#ncomp = plsda.cv()
#}
nipals.res <- plsda.nipals(X=X, y=ydum, ncomp=ncomp , max.iter=max.iter, tol=tol)
########################LDA########################
#ici on effectue la LDA pour la classification
#on l'a fait sur nos compossntes principales Th, obtenues en sorties de la PLS
Th <- nipals.res$comp_X
#Th<-t(apply(as.matrix(nipals.res$comp_X),1,function(ligne){ligne %*% t(as.matrix(nipals.res$poid_X))}))
#effectif par classe
n_k <- table(y) #train
#nombre d'individus
n <- nrow(Th)
#nombre de modalite
K <- nlevels(y)
#nombre de variables desc
p <- ncol(Th)
#proportion par classe
pi_k <- n_k / n
#calcul des moyennes conditionelles - lignes = classes
mb_k <- as.matrix(aggregate(Th,list(y),mean)[,2:(p+1)])
#calcul des matrices de covariances conditionnelles
V_k <- by(as.matrix(Th),list(y),cov)
#matrice de covariance intra-classe W
#calculée à partir des matrices conditionnelles V_k
W <- 1/(n-K) * Reduce("+",lapply(levels(y),function(k){(n_k[k]-1)*V_k[[k]]}))
#inverse de la matrice W
invW <- solve(W)
#calcul des coefficients des variables akj
#pour la fonction de classement
coef_ <- t(mb_k %*% invW)
coef_
colnames(coef_) <- levels(y)
intercept_ <- log(pi_k)-0.5*diag(mb_k %*% invW %*% t(mb_k))
#revenir a toutes les var originelles
coef_ <- as.matrix(nipals.res$poid_X)%*%coef_
coef_ <- diag(1/apply(X.init, 2, sd)) %*% coef_
coef_
intercept_ <- as.vector(-apply(X.init, 2, mean) %*% coef_) + log(pi_k)
res <- list("comp_X"= nipals.res$comp_X,
"poid_X" = nipals.res$poid_X,
"comp_Y" = nipals.res$comp_Y,
"poid_Y" = nipals.res$poid_Y,
"intercept_" = intercept_,
"coef_"=coef_,
"y" = y)
class(res)<-"PLSDA"
return(res)
}
data = read_excel("/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data = read_excel("C:/Users/pauli/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data = read_excel("C:/Users/pauli/Document/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data = read_excel("C:/Users/pauli/Documents/M2/R/projet/code/PLSDA_R_Package/Data_LDA_Python.xlsx")
data
res_fit = fit(TYPE~., data=data)
setwd("C:/Users/pauli/Documents/M2/R/projet/code/PLSDA_R_Package")
data = read_excel("/Data_LDA_Python.xlsx")
getwd()
data = read_excel("Data_LDA_Python.xlsx")
data
source("/code/fit.R")
source("code/fit.R")
source("code/dummies")
source("code/dummies.R")
source("code/cv.r")
source("code/nipals.r")
source("code/plot.r")
source("code/plot.R")
source("code/scale.R")
source("code/scale.R")
res_fit = fit(TYPE~., data=data)
res_fit
newdata = read_excel("Data_LDA_Python.xlsx", sheet="DATA_PREDCIT")
newdata = read_excel("Data_LDA_Python.xlsx", sheet="DATA_PREDICT")
pred = plsda.predict(res_fit,newdata)
source("predict.r")
source("code/predict.r")
pred = plsda.predict(res_fit,newdata)
pred
data
data_save = data
#alpha : risque pour piloter la sélection
alpha=0.05
lambda = 1
X <- as.matrix(model.matrix(formula, data = data)[,-1])
y <- as.factor(model.response(model.frame(formula, data = data)))
formula = TYPE~.
X <- as.matrix(model.matrix(formula, data = data)[,-1])
y <- as.factor(model.response(model.frame(formula, data = data)))
n <- nrow(data) # nombre d'observations
mod <- levels(y)
K <- nlevels(y) # nombre de classes
p <- ncol(X) # nombre de variables explicatives
n_k <- table(y) #effectifs des classes
lstInit <- colnames(X)
lambda_depart = 1
cov(X)
k
K
mod
n_k
